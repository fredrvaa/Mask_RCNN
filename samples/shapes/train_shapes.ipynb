{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask R-CNN - Train on Shapes Dataset\n",
    "\n",
    "\n",
    "This notebook shows how to train Mask R-CNN on your own dataset. To keep things simple we use a synthetic dataset of shapes (squares, triangles, and circles) which enables fast training. You'd still need a GPU, though, because the network backbone is a Resnet101, which would be too slow to train on a CPU. On a GPU, you can start to get okay-ish results in a few minutes, and good results in less than an hour.\n",
    "\n",
    "The code of the *Shapes* dataset is included below. It generates images on the fly, so it doesn't require downloading any data. And it can generate images of any size, so we pick a small image size to train faster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\DeepLearning3.6\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\Anaconda\\envs\\DeepLearning3.6\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\Anaconda\\envs\\DeepLearning3.6\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\Anaconda\\envs\\DeepLearning3.6\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\Anaconda\\envs\\DeepLearning3.6\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\Anaconda\\envs\\DeepLearning3.6\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "D:\\Anaconda\\envs\\DeepLearning3.6\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\Anaconda\\envs\\DeepLearning3.6\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\Anaconda\\envs\\DeepLearning3.6\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\Anaconda\\envs\\DeepLearning3.6\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\Anaconda\\envs\\DeepLearning3.6\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\Anaconda\\envs\\DeepLearning3.6\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"../../\")\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import utils\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn import visualize\n",
    "from mrcnn.model import log\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Local path to trained weights file\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
    "# Download COCO trained weights from Releases if needed\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "    utils.download_trained_weights(COCO_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE                       resnet101\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     8\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "COMPUTE_BACKBONE_SHAPE         None\n",
      "DETECTION_MAX_INSTANCES        100\n",
      "DETECTION_MIN_CONFIDENCE       0.7\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
      "GPU_COUNT                      1\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 8\n",
      "IMAGE_CHANNEL_COUNT            3\n",
      "IMAGE_MAX_DIM                  128\n",
      "IMAGE_META_SIZE                16\n",
      "IMAGE_MIN_DIM                  128\n",
      "IMAGE_MIN_SCALE                0\n",
      "IMAGE_RESIZE_MODE              square\n",
      "IMAGE_SHAPE                    [128 128   3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               100\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           shapes\n",
      "NUM_CLASSES                    4\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "PRE_NMS_LIMIT                  6000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (8, 16, 32, 64, 128)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                100\n",
      "TOP_DOWN_PYRAMID_SIZE          256\n",
      "TRAIN_BN                       False\n",
      "TRAIN_ROIS_PER_IMAGE           32\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               5\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class ShapesConfig(Config):\n",
    "    \"\"\"Configuration for training on the toy shapes dataset.\n",
    "    Derives from the base Config class and overrides values specific\n",
    "    to the toy shapes dataset.\n",
    "    \"\"\"\n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = \"shapes\"\n",
    "\n",
    "    # Train on 1 GPU and 8 images per GPU. We can put multiple images on each\n",
    "    # GPU because the images are small. Batch size is 8 (GPUs * images/GPU).\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 8\n",
    "\n",
    "    # Number of classes (including background)\n",
    "    NUM_CLASSES = 1 + 3  # background + 3 shapes\n",
    "\n",
    "    # Use small images for faster training. Set the limits of the small side\n",
    "    # the large side, and that determines the image shape.\n",
    "    IMAGE_MIN_DIM = 128\n",
    "    IMAGE_MAX_DIM = 128\n",
    "\n",
    "    # Use smaller anchors because our image and objects are small\n",
    "    RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)  # anchor side in pixels\n",
    "\n",
    "    # Reduce training ROIs per image because the images are small and have\n",
    "    # few objects. Aim to allow ROI sampling to pick 33% positive ROIs.\n",
    "    TRAIN_ROIS_PER_IMAGE = 32\n",
    "\n",
    "    # Use a small epoch since the data is simple\n",
    "    STEPS_PER_EPOCH = 100\n",
    "\n",
    "    # use small validation steps since the epoch is small\n",
    "    VALIDATION_STEPS = 5\n",
    "    \n",
    "config = ShapesConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ax(rows=1, cols=1, size=8):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "    \n",
    "    Change the default size attribute to control the size\n",
    "    of rendered images\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Create a synthetic dataset\n",
    "\n",
    "Extend the Dataset class and add a method to load the shapes dataset, `load_shapes()`, and override the following methods:\n",
    "\n",
    "* load_image()\n",
    "* load_mask()\n",
    "* image_reference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShapesDataset(utils.Dataset):\n",
    "    \"\"\"Generates the shapes synthetic dataset. The dataset consists of simple\n",
    "    shapes (triangles, squares, circles) placed randomly on a blank surface.\n",
    "    The images are generated on the fly. No file access required.\n",
    "    \"\"\"\n",
    "\n",
    "    def load_shapes(self, count, height, width):\n",
    "        \"\"\"Generate the requested number of synthetic images.\n",
    "        count: number of images to generate.\n",
    "        height, width: the size of the generated images.\n",
    "        \"\"\"\n",
    "        # Add classes\n",
    "        self.add_class(\"shapes\", 1, \"square\")\n",
    "        self.add_class(\"shapes\", 2, \"circle\")\n",
    "        self.add_class(\"shapes\", 3, \"triangle\")\n",
    "\n",
    "        # Add images\n",
    "        # Generate random specifications of images (i.e. color and\n",
    "        # list of shapes sizes and locations). This is more compact than\n",
    "        # actual images. Images are generated on the fly in load_image().\n",
    "        for i in range(count):\n",
    "            bg_color, shapes = self.random_image(height, width)\n",
    "            self.add_image(\"shapes\", image_id=i, path=None,\n",
    "                           width=width, height=height,\n",
    "                           bg_color=bg_color, shapes=shapes)\n",
    "\n",
    "    def load_image(self, image_id):\n",
    "        \"\"\"Generate an image from the specs of the given image ID.\n",
    "        Typically this function loads the image from a file, but\n",
    "        in this case it generates the image on the fly from the\n",
    "        specs in image_info.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        bg_color = np.array(info['bg_color']).reshape([1, 1, 3])\n",
    "        image = np.ones([info['height'], info['width'], 3], dtype=np.uint8)\n",
    "        image = image * bg_color.astype(np.uint8)\n",
    "        for shape, color, dims in info['shapes']:\n",
    "            image = self.draw_shape(image, shape, dims, color)\n",
    "        return image\n",
    "\n",
    "    def image_reference(self, image_id):\n",
    "        \"\"\"Return the shapes data of the image.\"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        if info[\"source\"] == \"shapes\":\n",
    "            return info[\"shapes\"]\n",
    "        else:\n",
    "            super(self.__class__).image_reference(self, image_id)\n",
    "\n",
    "    def load_mask(self, image_id):\n",
    "        \"\"\"Generate instance masks for shapes of the given image ID.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        shapes = info['shapes']\n",
    "        count = len(shapes)\n",
    "        mask = np.zeros([info['height'], info['width'], count], dtype=np.uint8)\n",
    "        for i, (shape, _, dims) in enumerate(info['shapes']):\n",
    "            mask[:, :, i:i+1] = self.draw_shape(mask[:, :, i:i+1].copy(),\n",
    "                                                shape, dims, 1)\n",
    "        # Handle occlusions\n",
    "        occlusion = np.logical_not(mask[:, :, -1]).astype(np.uint8)\n",
    "        for i in range(count-2, -1, -1):\n",
    "            mask[:, :, i] = mask[:, :, i] * occlusion\n",
    "            occlusion = np.logical_and(occlusion, np.logical_not(mask[:, :, i]))\n",
    "        # Map class names to class IDs.\n",
    "        class_ids = np.array([self.class_names.index(s[0]) for s in shapes])\n",
    "        return mask.astype(np.bool), class_ids.astype(np.int32)\n",
    "\n",
    "    def draw_shape(self, image, shape, dims, color):\n",
    "        \"\"\"Draws a shape from the given specs.\"\"\"\n",
    "        # Get the center x, y and the size s\n",
    "        x, y, s = dims\n",
    "        if shape == 'square':\n",
    "            cv2.rectangle(image, (x-s, y-s), (x+s, y+s), color, -1)\n",
    "        elif shape == \"circle\":\n",
    "            cv2.circle(image, (x, y), s, color, -1)\n",
    "        elif shape == \"triangle\":\n",
    "            points = np.array([[(x, y-s),\n",
    "                                (x-s/math.sin(math.radians(60)), y+s),\n",
    "                                (x+s/math.sin(math.radians(60)), y+s),\n",
    "                                ]], dtype=np.int32)\n",
    "            cv2.fillPoly(image, points, color)\n",
    "        return image\n",
    "\n",
    "    def random_shape(self, height, width):\n",
    "        \"\"\"Generates specifications of a random shape that lies within\n",
    "        the given height and width boundaries.\n",
    "        Returns a tuple of three valus:\n",
    "        * The shape name (square, circle, ...)\n",
    "        * Shape color: a tuple of 3 values, RGB.\n",
    "        * Shape dimensions: A tuple of values that define the shape size\n",
    "                            and location. Differs per shape type.\n",
    "        \"\"\"\n",
    "        # Shape\n",
    "        shape = random.choice([\"square\", \"circle\", \"triangle\"])\n",
    "        # Color\n",
    "        color = tuple([random.randint(0, 255) for _ in range(3)])\n",
    "        # Center x, y\n",
    "        buffer = 20\n",
    "        y = random.randint(buffer, height - buffer - 1)\n",
    "        x = random.randint(buffer, width - buffer - 1)\n",
    "        # Size\n",
    "        s = random.randint(buffer, height//4)\n",
    "        return shape, color, (x, y, s)\n",
    "\n",
    "    def random_image(self, height, width):\n",
    "        \"\"\"Creates random specifications of an image with multiple shapes.\n",
    "        Returns the background color of the image and a list of shape\n",
    "        specifications that can be used to draw the image.\n",
    "        \"\"\"\n",
    "        # Pick random background color\n",
    "        bg_color = np.array([random.randint(0, 255) for _ in range(3)])\n",
    "        # Generate a few random shapes and record their\n",
    "        # bounding boxes\n",
    "        shapes = []\n",
    "        boxes = []\n",
    "        N = random.randint(1, 4)\n",
    "        for _ in range(N):\n",
    "            shape, color, dims = self.random_shape(height, width)\n",
    "            shapes.append((shape, color, dims))\n",
    "            x, y, s = dims\n",
    "            boxes.append([y-s, x-s, y+s, x+s])\n",
    "        # Apply non-max suppression wit 0.3 threshold to avoid\n",
    "        # shapes covering each other\n",
    "        keep_ixs = utils.non_max_suppression(np.array(boxes), np.arange(N), 0.3)\n",
    "        shapes = [s for i, s in enumerate(shapes) if i in keep_ixs]\n",
    "        return bg_color, shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training dataset\n",
    "dataset_train = ShapesDataset()\n",
    "dataset_train.load_shapes(500, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "dataset_train.prepare()\n",
    "\n",
    "# Validation dataset\n",
    "dataset_val = ShapesDataset()\n",
    "dataset_val.load_shapes(50, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "dataset_val.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load and display random samples\n",
    "image_ids = np.random.choice(dataset_train.image_ids, 4)\n",
    "for image_id in image_ids:\n",
    "    image = dataset_train.load_image(image_id)\n",
    "    mask, class_ids = dataset_train.load_mask(image_id)\n",
    "    visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda\\envs\\DeepLearning3.6\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda\\envs\\DeepLearning3.6\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda\\envs\\DeepLearning3.6\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda\\envs\\DeepLearning3.6\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2139: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda\\envs\\DeepLearning3.6\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda\\envs\\DeepLearning3.6\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2239: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda\\envs\\DeepLearning3.6\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From D:\\Anaconda\\envs\\DeepLearning3.6\\lib\\site-packages\\mask_rcnn-2.1-py3.6.egg\\mrcnn\\model.py:553: The name tf.random_shuffle is deprecated. Please use tf.random.shuffle instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda\\envs\\DeepLearning3.6\\lib\\site-packages\\mask_rcnn-2.1-py3.6.egg\\mrcnn\\utils.py:202: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda\\envs\\DeepLearning3.6\\lib\\site-packages\\mask_rcnn-2.1-py3.6.egg\\mrcnn\\model.py:600: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "box_ind is deprecated, use box_indices instead\n"
     ]
    }
   ],
   "source": [
    "# Create model in training mode\n",
    "model = modellib.MaskRCNN(mode=\"training\", config=config,\n",
    "                          model_dir=MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Which weights to start with?\n",
    "init_with = \"coco\"  # imagenet, coco, or last\n",
    "\n",
    "if init_with == \"imagenet\":\n",
    "    model.load_weights(model.get_imagenet_weights(), by_name=True)\n",
    "elif init_with == \"coco\":\n",
    "    # Load weights trained on MS COCO, but skip layers that\n",
    "    # are different due to the different number of classes\n",
    "    # See README for instructions to download the COCO weights\n",
    "    model.load_weights(COCO_MODEL_PATH, by_name=True,\n",
    "                       exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \n",
    "                                \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "elif init_with == \"last\":\n",
    "    # Load the last model you trained and continue training\n",
    "    model.load_weights(model.find_last(), by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Train in two stages:\n",
    "1. Only the heads. Here we're freezing all the backbone layers and training only the randomly initialized layers (i.e. the ones that we didn't use pre-trained weights from MS COCO). To train only the head layers, pass `layers='heads'` to the `train()` function.\n",
    "\n",
    "2. Fine-tune all layers. For this simple example it's not necessary, but we're including it to show the process. Simply pass `layers=\"all` to train all layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting at epoch 0. LR=0.001\n",
      "\n",
      "Checkpoint Path: C:\\Users\\Fredrik\\Desktop\\Mask_RCNN\\logs\\shapes20190913T1723\\mask_rcnn_shapes_{epoch:04d}.h5\n",
      "Selecting layers to train\n",
      "fpn_c5p5               (Conv2D)\n",
      "fpn_c4p4               (Conv2D)\n",
      "fpn_c3p3               (Conv2D)\n",
      "fpn_c2p2               (Conv2D)\n",
      "fpn_p5                 (Conv2D)\n",
      "fpn_p2                 (Conv2D)\n",
      "fpn_p3                 (Conv2D)\n",
      "fpn_p4                 (Conv2D)\n",
      "In model:  rpn_model\n",
      "    rpn_conv_shared        (Conv2D)\n",
      "    rpn_class_raw          (Conv2D)\n",
      "    rpn_bbox_pred          (Conv2D)\n",
      "mrcnn_mask_conv1       (TimeDistributed)\n",
      "mrcnn_mask_bn1         (TimeDistributed)\n",
      "mrcnn_mask_conv2       (TimeDistributed)\n",
      "mrcnn_mask_bn2         (TimeDistributed)\n",
      "mrcnn_class_conv1      (TimeDistributed)\n",
      "mrcnn_class_bn1        (TimeDistributed)\n",
      "mrcnn_mask_conv3       (TimeDistributed)\n",
      "mrcnn_mask_bn3         (TimeDistributed)\n",
      "mrcnn_class_conv2      (TimeDistributed)\n",
      "mrcnn_class_bn2        (TimeDistributed)\n",
      "mrcnn_mask_conv4       (TimeDistributed)\n",
      "mrcnn_mask_bn4         (TimeDistributed)\n",
      "mrcnn_bbox_fc          (TimeDistributed)\n",
      "mrcnn_mask_deconv      (TimeDistributed)\n",
      "mrcnn_class_logits     (TimeDistributed)\n",
      "mrcnn_mask             (TimeDistributed)\n",
      "WARNING:tensorflow:From D:\\Anaconda\\envs\\DeepLearning3.6\\lib\\site-packages\\keras\\optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\DeepLearning3.6\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "D:\\Anaconda\\envs\\DeepLearning3.6\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "D:\\Anaconda\\envs\\DeepLearning3.6\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda\\envs\\DeepLearning3.6\\lib\\site-packages\\keras\\callbacks.py:1122: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda\\envs\\DeepLearning3.6\\lib\\site-packages\\keras\\callbacks.py:1125: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 83s 829ms/step - loss: 1.8243 - rpn_class_loss: 0.0326 - rpn_bbox_loss: 0.6257 - mrcnn_class_loss: 0.3321 - mrcnn_bbox_loss: 0.4103 - mrcnn_mask_loss: 0.4235 - val_loss: 0.8891 - val_rpn_class_loss: 0.0178 - val_rpn_bbox_loss: 0.4626 - val_mrcnn_class_loss: 0.1007 - val_mrcnn_bbox_loss: 0.1533 - val_mrcnn_mask_loss: 0.1546\n"
     ]
    }
   ],
   "source": [
    "# Train the head branches\n",
    "# Passing layers=\"heads\" freezes all layers except the head\n",
    "# layers. You can also pass a regular expression to select\n",
    "# which layers to train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE, \n",
    "            epochs=1, \n",
    "            layers='heads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting at epoch 1. LR=0.0001\n",
      "\n",
      "Checkpoint Path: C:\\Users\\Fredrik\\Desktop\\Mask_RCNN\\logs\\shapes20190913T1723\\mask_rcnn_shapes_{epoch:04d}.h5\n",
      "Selecting layers to train\n",
      "conv1                  (Conv2D)\n",
      "bn_conv1               (BatchNorm)\n",
      "res2a_branch2a         (Conv2D)\n",
      "bn2a_branch2a          (BatchNorm)\n",
      "res2a_branch2b         (Conv2D)\n",
      "bn2a_branch2b          (BatchNorm)\n",
      "res2a_branch2c         (Conv2D)\n",
      "res2a_branch1          (Conv2D)\n",
      "bn2a_branch2c          (BatchNorm)\n",
      "bn2a_branch1           (BatchNorm)\n",
      "res2b_branch2a         (Conv2D)\n",
      "bn2b_branch2a          (BatchNorm)\n",
      "res2b_branch2b         (Conv2D)\n",
      "bn2b_branch2b          (BatchNorm)\n",
      "res2b_branch2c         (Conv2D)\n",
      "bn2b_branch2c          (BatchNorm)\n",
      "res2c_branch2a         (Conv2D)\n",
      "bn2c_branch2a          (BatchNorm)\n",
      "res2c_branch2b         (Conv2D)\n",
      "bn2c_branch2b          (BatchNorm)\n",
      "res2c_branch2c         (Conv2D)\n",
      "bn2c_branch2c          (BatchNorm)\n",
      "res3a_branch2a         (Conv2D)\n",
      "bn3a_branch2a          (BatchNorm)\n",
      "res3a_branch2b         (Conv2D)\n",
      "bn3a_branch2b          (BatchNorm)\n",
      "res3a_branch2c         (Conv2D)\n",
      "res3a_branch1          (Conv2D)\n",
      "bn3a_branch2c          (BatchNorm)\n",
      "bn3a_branch1           (BatchNorm)\n",
      "res3b_branch2a         (Conv2D)\n",
      "bn3b_branch2a          (BatchNorm)\n",
      "res3b_branch2b         (Conv2D)\n",
      "bn3b_branch2b          (BatchNorm)\n",
      "res3b_branch2c         (Conv2D)\n",
      "bn3b_branch2c          (BatchNorm)\n",
      "res3c_branch2a         (Conv2D)\n",
      "bn3c_branch2a          (BatchNorm)\n",
      "res3c_branch2b         (Conv2D)\n",
      "bn3c_branch2b          (BatchNorm)\n",
      "res3c_branch2c         (Conv2D)\n",
      "bn3c_branch2c          (BatchNorm)\n",
      "res3d_branch2a         (Conv2D)\n",
      "bn3d_branch2a          (BatchNorm)\n",
      "res3d_branch2b         (Conv2D)\n",
      "bn3d_branch2b          (BatchNorm)\n",
      "res3d_branch2c         (Conv2D)\n",
      "bn3d_branch2c          (BatchNorm)\n",
      "res4a_branch2a         (Conv2D)\n",
      "bn4a_branch2a          (BatchNorm)\n",
      "res4a_branch2b         (Conv2D)\n",
      "bn4a_branch2b          (BatchNorm)\n",
      "res4a_branch2c         (Conv2D)\n",
      "res4a_branch1          (Conv2D)\n",
      "bn4a_branch2c          (BatchNorm)\n",
      "bn4a_branch1           (BatchNorm)\n",
      "res4b_branch2a         (Conv2D)\n",
      "bn4b_branch2a          (BatchNorm)\n",
      "res4b_branch2b         (Conv2D)\n",
      "bn4b_branch2b          (BatchNorm)\n",
      "res4b_branch2c         (Conv2D)\n",
      "bn4b_branch2c          (BatchNorm)\n",
      "res4c_branch2a         (Conv2D)\n",
      "bn4c_branch2a          (BatchNorm)\n",
      "res4c_branch2b         (Conv2D)\n",
      "bn4c_branch2b          (BatchNorm)\n",
      "res4c_branch2c         (Conv2D)\n",
      "bn4c_branch2c          (BatchNorm)\n",
      "res4d_branch2a         (Conv2D)\n",
      "bn4d_branch2a          (BatchNorm)\n",
      "res4d_branch2b         (Conv2D)\n",
      "bn4d_branch2b          (BatchNorm)\n",
      "res4d_branch2c         (Conv2D)\n",
      "bn4d_branch2c          (BatchNorm)\n",
      "res4e_branch2a         (Conv2D)\n",
      "bn4e_branch2a          (BatchNorm)\n",
      "res4e_branch2b         (Conv2D)\n",
      "bn4e_branch2b          (BatchNorm)\n",
      "res4e_branch2c         (Conv2D)\n",
      "bn4e_branch2c          (BatchNorm)\n",
      "res4f_branch2a         (Conv2D)\n",
      "bn4f_branch2a          (BatchNorm)\n",
      "res4f_branch2b         (Conv2D)\n",
      "bn4f_branch2b          (BatchNorm)\n",
      "res4f_branch2c         (Conv2D)\n",
      "bn4f_branch2c          (BatchNorm)\n",
      "res4g_branch2a         (Conv2D)\n",
      "bn4g_branch2a          (BatchNorm)\n",
      "res4g_branch2b         (Conv2D)\n",
      "bn4g_branch2b          (BatchNorm)\n",
      "res4g_branch2c         (Conv2D)\n",
      "bn4g_branch2c          (BatchNorm)\n",
      "res4h_branch2a         (Conv2D)\n",
      "bn4h_branch2a          (BatchNorm)\n",
      "res4h_branch2b         (Conv2D)\n",
      "bn4h_branch2b          (BatchNorm)\n",
      "res4h_branch2c         (Conv2D)\n",
      "bn4h_branch2c          (BatchNorm)\n",
      "res4i_branch2a         (Conv2D)\n",
      "bn4i_branch2a          (BatchNorm)\n",
      "res4i_branch2b         (Conv2D)\n",
      "bn4i_branch2b          (BatchNorm)\n",
      "res4i_branch2c         (Conv2D)\n",
      "bn4i_branch2c          (BatchNorm)\n",
      "res4j_branch2a         (Conv2D)\n",
      "bn4j_branch2a          (BatchNorm)\n",
      "res4j_branch2b         (Conv2D)\n",
      "bn4j_branch2b          (BatchNorm)\n",
      "res4j_branch2c         (Conv2D)\n",
      "bn4j_branch2c          (BatchNorm)\n",
      "res4k_branch2a         (Conv2D)\n",
      "bn4k_branch2a          (BatchNorm)\n",
      "res4k_branch2b         (Conv2D)\n",
      "bn4k_branch2b          (BatchNorm)\n",
      "res4k_branch2c         (Conv2D)\n",
      "bn4k_branch2c          (BatchNorm)\n",
      "res4l_branch2a         (Conv2D)\n",
      "bn4l_branch2a          (BatchNorm)\n",
      "res4l_branch2b         (Conv2D)\n",
      "bn4l_branch2b          (BatchNorm)\n",
      "res4l_branch2c         (Conv2D)\n",
      "bn4l_branch2c          (BatchNorm)\n",
      "res4m_branch2a         (Conv2D)\n",
      "bn4m_branch2a          (BatchNorm)\n",
      "res4m_branch2b         (Conv2D)\n",
      "bn4m_branch2b          (BatchNorm)\n",
      "res4m_branch2c         (Conv2D)\n",
      "bn4m_branch2c          (BatchNorm)\n",
      "res4n_branch2a         (Conv2D)\n",
      "bn4n_branch2a          (BatchNorm)\n",
      "res4n_branch2b         (Conv2D)\n",
      "bn4n_branch2b          (BatchNorm)\n",
      "res4n_branch2c         (Conv2D)\n",
      "bn4n_branch2c          (BatchNorm)\n",
      "res4o_branch2a         (Conv2D)\n",
      "bn4o_branch2a          (BatchNorm)\n",
      "res4o_branch2b         (Conv2D)\n",
      "bn4o_branch2b          (BatchNorm)\n",
      "res4o_branch2c         (Conv2D)\n",
      "bn4o_branch2c          (BatchNorm)\n",
      "res4p_branch2a         (Conv2D)\n",
      "bn4p_branch2a          (BatchNorm)\n",
      "res4p_branch2b         (Conv2D)\n",
      "bn4p_branch2b          (BatchNorm)\n",
      "res4p_branch2c         (Conv2D)\n",
      "bn4p_branch2c          (BatchNorm)\n",
      "res4q_branch2a         (Conv2D)\n",
      "bn4q_branch2a          (BatchNorm)\n",
      "res4q_branch2b         (Conv2D)\n",
      "bn4q_branch2b          (BatchNorm)\n",
      "res4q_branch2c         (Conv2D)\n",
      "bn4q_branch2c          (BatchNorm)\n",
      "res4r_branch2a         (Conv2D)\n",
      "bn4r_branch2a          (BatchNorm)\n",
      "res4r_branch2b         (Conv2D)\n",
      "bn4r_branch2b          (BatchNorm)\n",
      "res4r_branch2c         (Conv2D)\n",
      "bn4r_branch2c          (BatchNorm)\n",
      "res4s_branch2a         (Conv2D)\n",
      "bn4s_branch2a          (BatchNorm)\n",
      "res4s_branch2b         (Conv2D)\n",
      "bn4s_branch2b          (BatchNorm)\n",
      "res4s_branch2c         (Conv2D)\n",
      "bn4s_branch2c          (BatchNorm)\n",
      "res4t_branch2a         (Conv2D)\n",
      "bn4t_branch2a          (BatchNorm)\n",
      "res4t_branch2b         (Conv2D)\n",
      "bn4t_branch2b          (BatchNorm)\n",
      "res4t_branch2c         (Conv2D)\n",
      "bn4t_branch2c          (BatchNorm)\n",
      "res4u_branch2a         (Conv2D)\n",
      "bn4u_branch2a          (BatchNorm)\n",
      "res4u_branch2b         (Conv2D)\n",
      "bn4u_branch2b          (BatchNorm)\n",
      "res4u_branch2c         (Conv2D)\n",
      "bn4u_branch2c          (BatchNorm)\n",
      "res4v_branch2a         (Conv2D)\n",
      "bn4v_branch2a          (BatchNorm)\n",
      "res4v_branch2b         (Conv2D)\n",
      "bn4v_branch2b          (BatchNorm)\n",
      "res4v_branch2c         (Conv2D)\n",
      "bn4v_branch2c          (BatchNorm)\n",
      "res4w_branch2a         (Conv2D)\n",
      "bn4w_branch2a          (BatchNorm)\n",
      "res4w_branch2b         (Conv2D)\n",
      "bn4w_branch2b          (BatchNorm)\n",
      "res4w_branch2c         (Conv2D)\n",
      "bn4w_branch2c          (BatchNorm)\n",
      "res5a_branch2a         (Conv2D)\n",
      "bn5a_branch2a          (BatchNorm)\n",
      "res5a_branch2b         (Conv2D)\n",
      "bn5a_branch2b          (BatchNorm)\n",
      "res5a_branch2c         (Conv2D)\n",
      "res5a_branch1          (Conv2D)\n",
      "bn5a_branch2c          (BatchNorm)\n",
      "bn5a_branch1           (BatchNorm)\n",
      "res5b_branch2a         (Conv2D)\n",
      "bn5b_branch2a          (BatchNorm)\n",
      "res5b_branch2b         (Conv2D)\n",
      "bn5b_branch2b          (BatchNorm)\n",
      "res5b_branch2c         (Conv2D)\n",
      "bn5b_branch2c          (BatchNorm)\n",
      "res5c_branch2a         (Conv2D)\n",
      "bn5c_branch2a          (BatchNorm)\n",
      "res5c_branch2b         (Conv2D)\n",
      "bn5c_branch2b          (BatchNorm)\n",
      "res5c_branch2c         (Conv2D)\n",
      "bn5c_branch2c          (BatchNorm)\n",
      "fpn_c5p5               (Conv2D)\n",
      "fpn_c4p4               (Conv2D)\n",
      "fpn_c3p3               (Conv2D)\n",
      "fpn_c2p2               (Conv2D)\n",
      "fpn_p5                 (Conv2D)\n",
      "fpn_p2                 (Conv2D)\n",
      "fpn_p3                 (Conv2D)\n",
      "fpn_p4                 (Conv2D)\n",
      "In model:  rpn_model\n",
      "    rpn_conv_shared        (Conv2D)\n",
      "    rpn_class_raw          (Conv2D)\n",
      "    rpn_bbox_pred          (Conv2D)\n",
      "mrcnn_mask_conv1       (TimeDistributed)\n",
      "mrcnn_mask_bn1         (TimeDistributed)\n",
      "mrcnn_mask_conv2       (TimeDistributed)\n",
      "mrcnn_mask_bn2         (TimeDistributed)\n",
      "mrcnn_class_conv1      (TimeDistributed)\n",
      "mrcnn_class_bn1        (TimeDistributed)\n",
      "mrcnn_mask_conv3       (TimeDistributed)\n",
      "mrcnn_mask_bn3         (TimeDistributed)\n",
      "mrcnn_class_conv2      (TimeDistributed)\n",
      "mrcnn_class_bn2        (TimeDistributed)\n",
      "mrcnn_mask_conv4       (TimeDistributed)\n",
      "mrcnn_mask_bn4         (TimeDistributed)\n",
      "mrcnn_bbox_fc          (TimeDistributed)\n",
      "mrcnn_mask_deconv      (TimeDistributed)\n",
      "mrcnn_class_logits     (TimeDistributed)\n",
      "mrcnn_mask             (TimeDistributed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\DeepLearning3.6\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "D:\\Anaconda\\envs\\DeepLearning3.6\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "D:\\Anaconda\\envs\\DeepLearning3.6\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2\n",
      "100/100 [==============================] - 111s 1s/step - loss: 0.8736 - rpn_class_loss: 0.0176 - rpn_bbox_loss: 0.4205 - mrcnn_class_loss: 0.1159 - mrcnn_bbox_loss: 0.1357 - mrcnn_mask_loss: 0.1839 - val_loss: 0.8872 - val_rpn_class_loss: 0.0196 - val_rpn_bbox_loss: 0.4469 - val_mrcnn_class_loss: 0.0945 - val_mrcnn_bbox_loss: 0.1373 - val_mrcnn_mask_loss: 0.1889\n"
     ]
    }
   ],
   "source": [
    "# Fine tune all layers\n",
    "# Passing layers=\"all\" trains all layers. You can also \n",
    "# pass a regular expression to select which layers to\n",
    "# train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE / 10,\n",
    "            epochs=2, \n",
    "            layers=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weights\n",
    "# Typically not needed because callbacks save after every epoch\n",
    "# Uncomment to save manually\n",
    "# model_path = os.path.join(MODEL_DIR, \"mask_rcnn_shapes.h5\")\n",
    "# model.keras_model.save_weights(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda\\envs\\DeepLearning3.6\\lib\\site-packages\\mask_rcnn-2.1-py3.6.egg\\mrcnn\\model.py:720: The name tf.sets.set_intersection is deprecated. Please use tf.sets.intersection instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda\\envs\\DeepLearning3.6\\lib\\site-packages\\mask_rcnn-2.1-py3.6.egg\\mrcnn\\model.py:772: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "Loading weights from  C:\\Users\\Fredrik\\Desktop\\Mask_RCNN\\logs\\shapes20190913T1723\\mask_rcnn_shapes_0002.h5\n",
      "Re-starting from epoch 2\n"
     ]
    }
   ],
   "source": [
    "class InferenceConfig(ShapesConfig):\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "inference_config = InferenceConfig()\n",
    "\n",
    "# Recreate the model in inference mode\n",
    "model = modellib.MaskRCNN(mode=\"inference\", \n",
    "                          config=inference_config,\n",
    "                          model_dir=MODEL_DIR)\n",
    "\n",
    "# Get path to saved weights\n",
    "# Either set a specific path or find last trained weights\n",
    "# model_path = os.path.join(ROOT_DIR, \".h5 file name here\")\n",
    "model_path = model.find_last()\n",
    "\n",
    "# Load trained weights\n",
    "print(\"Loading weights from \", model_path)\n",
    "model.load_weights(model_path, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original_image           shape: (128, 128, 3)         min:   31.00000  max:  214.00000  uint8\n",
      "image_meta               shape: (16,)                 min:    0.00000  max:  128.00000  int32\n",
      "gt_class_id              shape: (1,)                  min:    1.00000  max:    1.00000  int32\n",
      "gt_bbox                  shape: (1, 4)                min:   13.00000  max:  128.00000  int32\n",
      "gt_mask                  shape: (128, 128, 1)         min:    0.00000  max:    1.00000  bool\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcEAAAHBCAYAAAARuwDoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAP8ElEQVR4nO3da2yd9X3A8d85dhwHBycQIAmJAwmXBOrAKgYJuZQxukGhiLIFlF26aZq6F9s0IQ2kqZOqSdXairEuqioYL7ppm7QxNetopTBgYyDFoQHUbkko15CQCyFJE8VO7PgS22cvkrmU1snJOQee2L/P59Wx8/c5vyeR8n1uelyqVCoBABmVix4AAIoiggCkJYIApCWCAKQlggCkJYIApCWCAKQlggCkJYIApCWCAKQlggCkJYIApCWCAKQlggCkJYIApCWCAKQlggCkJYIApCWCAKQlggCkJYIApCWCAKQlggCkJYIApCWCAKQlggCkJYIApCWCAKQlggCkJYIApCWCAKQlggCkJYIApNVc9AAfdP2aVZWiZwDgo7FlfVep6Bk+zJEgAGmJIABpiSAAaYkgAGmJIABpiSAAaYkgAGmJIABpiSAAaYkgAGmJIABpiSAAaYkgAGmJIABpiSAAaYkgAGmJIABpiSAAaYkgAGmJIABpiSAAaYkgAGmJIABpiSAAaYkgAGmJIABpiSAAaYkgAGmJIABpiSAAaYkgAGmJIABpiSAAaYkgAGmJIABpiSAAaYkgAGmJIABpiSAAaYkgAGmJIABpiSAAaYkgAGmJIABpiSAAaYkgAGmJIABpiSAAaYkgAGmJIABpiSAAaYkgAGmJIABpiSAAaYkgAGmJIABpiSAAaYkgAGmJIABpiSAAaYkgAGmJIABpiSAAaYkgAGmJIABpiSAAaYkgAGmJIABpiSAAaYkgAGmJIABpiSAAaYkgAGmJIABpiSAAaYkgAGmJIABpiSAAaYkgAGmJIABpiSAAaYkgAGmJIABpiSAAaYkgAGmJIABpiSAAaYkgAGmJIABpiSAAaYkgAGmJIABpiSAAaYkgAGmJIABpiSAAaYkgAGmJIABpiSAAaYkgAGmJIABpiSAAaYkgAGmJIABpiSAAaYkgAGmJIABpiSAAaYkgAGmJIABpiSAAaYkgAGmJIABpiSAAaYkgAGmJIABpiSAAaYkgAGmJIABpiSAAaYkgAGmJIABpiSAAaYkgAGmJIABpiSAAaYkgAGmJIABpiSAAaYkgAGmJIABpiSAAaYkgAGmJIABpiSAAaYkgAGmJIABpiSAAaYkgAGmJIABpiSAAaYkgAGmJIABpiSAAaYkgAGmJIABpiSAAaYkgAGmJIABpiSAAaYkgAGmJIABpiSAAaYkgAGmJIABpiSAAaYkgAGmJIABpiSAAaYkgAGmJIABpiSAAaYkgAGmJIABpiSAAaYkgAGmJIABpiSAAaYkgAGmJIABpiSAAaYkgAGmJIABpiSAAaYkgAGmJIABpiSAAaYkgAGmJIABpiSAAaYkgAGmJIABpiSAAaYkgAGmJIABpiSAAaYkgAGmJIABpiSAAaYkgAGmJIABpiSAAaYkgAGmJIABpiSAAaYkgAGmJIABpiSAAaYkgAGmJIABpiSAAaYkgAGmJIABpiSAAaYkgAGmJIABpiSAAaYkgAGmJIABpiSAAaYkgAGmJIABpiSAAaYkgAGmJIABpiSAAaYkgAGmJIABpiSAAaYkgAGmJIABpiSAAaYkgAGmJIABpiSAAaYkgAGmJIABpiSAAaYkgAGmJIABpiSAAaYkgAGmJIABpiSAAaYkgAGmJIABpiSAAaYkgAGmJIABpiSAAaYkgAGmJIABpiSAAaYkgAGmJIABpiSAAaYkgAGmJIABpiSAAaTUXPcBE8Mf/eW3cfvzpmDVyuOhRKNie5o54Ydqt8divbCt6FKABRPAMWkZG4+6+78WhpoujVKmMu+71liWxr3l+RERcOrw3rhl6Y9y1z5336bHXNw68FO2jx37uuvea58UbLddERMT5I0fjpsGXx33Pl6feFMea2iMiYsnQ6zFv+L2fu+5o+fx4pXXZ2Ne3Hf+vcd/TNv3sNi0Y3h2fP/aPcU1XS0SpFH+y8tpxfx4494ngGSw8djymVgZj+sixGCmN/9c1WGqN4+W2sdenW/v/6yIihktTxl07VGoZW9tcOXHa9+wvTxtbO1RqGXftcGnKT32+bTq7bfpx+eJYMLI7ytESo+P+JDBRlCqnObr5uF2/ZtW5M8wpi7t749Hn90Zv+fyIiHiq7a6CJ6JoX+h5PDbPnh6jjgThrGxZ31UqeoYPcyQINdh8ycwYLruvDCY6EYQarF80Jwaam4oeA6iTXVkA0hJBqMH8voHo6O0vegygTiIINXhg27vx0JadRY8B1Mk1wSp1ta4segQAGkwEq3S0aWbRIwDQYE6HApCWCFapc3BrdA5uLXoMABpIBKu0YHhPLBjeU/QYADSQCAKQlhtjoAbrll4eg032IWGiE0Gowd62Vo9Ng0nAriwAaYkg1GDNjv2xdvv7RY8B1EkEq9RTbo+ecvvH/rkl153OScsPdseKA0eKHgOok2uCVdo0bXU0T5sSv/rY5+PCJXNjdHgkut8+GM/+3t/FTX/+2bjq12+I3ve74+APdsW81VfH+lsfjsW/uSwuv70znvndb0VE/NTXF157aXzqkftjStvUaJraHK/9w6bY+tgLERHxy4/+dgz1DsbMRRdH60XTY/0vPRyX3HBZ3PwX98SU81sjIuKVr2yIXc/+qKi/DoBJQQTPQsdt18TUmefFE8v/MiIips6YFpfd0RkL71wa/7r6qzHSfyI+889/UNV7Hdt9OL73uW/G6NBwNLe1xJrnHoo9z70eR946EBERc25cGE/etS6Gjw9Fy4xpccvfrI0N9z0Wxw8cjfNmt8ea/34onljxlRjq8ZsMAGolgmfh8Lb34oKrZsfqv7o/9nW9HbuefTXmrb46tn/nhzHcNxQREa//0/fjhgfvOON7NU9riVv++t6Y1Tk/KpXRaJs7I2Z1zh+L4Dvf/Z8YPn7yPefctDDaL5sVn13/hz95g0rEjIUXx4//d3fjNxQgCRGs0p19GyJei/iXZb0x/5bFseDT18ayL90du555ddyfqQyPRqlcGvu6eeqUsdfLv3R3HD94LJ771NeiMjIad3/nj6Kp9Sf/HCf6Bsdel0qlOPyjffHknesavFUAubnr4iw0zZsTlZHR2Llha2z64r/FtIumx6Gte+OKez8Zzee1RKlciiW/tXxsfc/OQzHrE/Oi3NIc5SlNseieXxj7s5YZ50Xve0eiMjIaF14zN+befMW4n7v/pR0xY9HFcenqq8a+d8knF3w0GwmQiCPBs9DSuTh+7Wt/GhERpXI5fvj1Z+PNJ16OmVfOjvs3/ln07e+JfRvfjra5J3/t0oFXdsbeF96Mtd//YhzbdTiOvHUg2mafvMP0B488Hbf97e/E1fffGD07D8W+F98Z93MHe/rjP37j8bj5y/fG1K9Oi6YpzXH03UOxYe3jEZXKR7/h/Iy9ba0x8oGjfGBiKlXOof9Er1+z6twZ5pTF3b3x6PN7o7d8fkREPNV212nXX7rqqljx5Xtj/a0PfxzjUYAv9Dweq+75RU+MgbO0ZX3XObfn6HQoAGmJYIPt63rbUSDABCGCUINHNr8R39j0WtFjAHVyY0yVtrV0Fj0CAA0mglXaM+WyokcAoMGcDgUgLRGsUseJXdFxYlfRYwDQQE6HVmnp0MnHozktCjB5OBIEIC1HglCDby+cEyf8wmOY8EQQavDS7JkemwaTgF1ZANISQajBsgPdsWL/kaLHAOrkdCjU4L6d+2O0VIoX51xQ9ChAHUSwSmf6FUoATDxOhwKQlggCkJYIVmll/8ZY2b+x6DEAaCDXBKs0Y/Ro0SMA0GCOBAFIy5Eg1ODB5Us8MQYmAUeCAKQlggCkJYJQgwe2vRsPbdlR9BhAnVwTrNLu5o6iR+AcMr9vIEZLpaLHAOokglV6dep1RY8AQIM5HQpAWiJYpfaR7mgf6S56DAAaSASrtGpgU6wa2FT0GAA0kAgCkJYbY6AGmy+ZGcNl+5Aw0Ykg1GD9ojkemwaTgF1ZANISQajB/L6B6OjtL3oMoE4iCDU4+di0nUWPAdTJNcEqdbWuLHoEABpMBKt0tGlm0SMA0GBOhwKQlghWqXNwa3QObi16DAAaSASrtGB4TywY3lP0GAA0kAgCkJYbY6AG65ZeHoNN9iFhohNBqMHetlaPTYNJwK4sAGmJINRgzY79sXb7+0WPAdRJBKvUU26PnnJ70WNwjlh+sDtWHDhS9BhAnVwTrNKmaauLHgGABnMkCEBaIghAWiJYpTv7NsSdfRuKHgOABhJBANJyYwzUYG9ba4yUS0WPAdRJBKEG65Ze7okxMAk4HQpAWiIIQFoiCDV4ZPMb8Y1NrxU9BlAn1wSrtK2ls+gRAGgwEazSnimXFT0CAA3mdCgAaYlglTpO7IqOE7uKHgOABnI6tEpLh16NCKdFASYTR4IApOVIEGrw7YVz4kSTfUiY6EQQavDS7JkemwaTgF1ZANISQajBsgPdsWL/kaLHAOrkdCjU4L6d+2O0VIoX51xQ9ChAHUSwSk+13VX0CAA0mNOhAKQlggCkJYJVWtm/MVb2byx6DAAayDXBKs0YPVr0CAA0mCNBANJyJAg1eHD5Ek+MgUnAkSAAaYkgAGmJINTggW3vxkNbdhQ9BlAn1wSrtLu5o+gROIfM7xuI0VKp6DGAOolglV6del3RIwDQYE6HApCWCFapfaQ72ke6ix4DgAYSwSqtGtgUqwY2FT0GAA0kggCk5cYYqMHmS2bGcNk+JEx0Igg1WL9ojsemwSRgVxaAtEQQajC/byA6evuLHgOokwhCDU4+Nm1n0WMAdXJNsEpdrSuLHgGABhPBKh1tmln0CAA0mNOhAKQlglXqHNwanYNbix4DgAYSwSotGN4TC4b3FD0GAA0kggCk5cYYqMG6pZfHYJN9SJjoRBBqsLet1WPTYBKwKwtAWiIINVizY3+s3f5+0WMAdRLBM+htbo7pld7oL02NnnJ70eNQsLnD+6ISpVh2sDtWHDhS9DhAnVwTPIP3prfGi60r4qaBl2P7lCtjRX9X0SNRkHJU4qoTb8XTbZ+JGaWNRY8DNECpUqkUPcPEUCrdHhGfiIjfP82qroh489TrxRGx6jRrv/WB1/dExEXjrHsjIjadej0rIj53mvd8MiIOn3q9MiKWjLPuUER89wNf26bxfXibyhHRPfadSuXu0/wscI5zJFitSuWZiHgmSqVbT7Pq36NSeToiIkqlOyLi0tO839fHXpdKCyLiinFWPh+VyjdPrbsyIq47zef/fVQq20+tHYqIlnHWvfOhz7dN46l+m4AJyJEgAGm5MQaAtEQQgLREEIC0RBCAtEQQgLREEIC0RBCAtEQQgLREEIC0RBCAtEQQgLREEIC0RBCAtEQQgLREEIC0RBCAtEQQgLREEIC0RBCAtEQQgLREEIC0RBCAtEQQgLREEIC0RBCAtEQQgLREEIC0RBCAtEQQgLREEIC0RBCAtEQQgLREEIC0RBCAtEQQgLT+D7AG4Ja+2SkuAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test on a random image\n",
    "image_id = random.choice(dataset_val.image_ids)\n",
    "original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "    modellib.load_image_gt(dataset_val, inference_config, \n",
    "                           image_id, use_mini_mask=False)\n",
    "\n",
    "log(\"original_image\", original_image)\n",
    "log(\"image_meta\", image_meta)\n",
    "log(\"gt_class_id\", gt_class_id)\n",
    "log(\"gt_bbox\", gt_bbox)\n",
    "log(\"gt_mask\", gt_mask)\n",
    "\n",
    "visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n",
    "                            dataset_train.class_names, figsize=(8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1 images\n",
      "image                    shape: (128, 128, 3)         min:   31.00000  max:  214.00000  uint8\n",
      "molded_images            shape: (1, 128, 128, 3)      min:  -85.80000  max:  110.10000  float64\n",
      "image_metas              shape: (1, 16)               min:    0.00000  max:  128.00000  int32\n",
      "anchors                  shape: (1, 4092, 4)          min:   -0.71267  max:    1.20874  float32\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = model.detect([original_image], verbose=1)\n",
    "\n",
    "r = results[0]\n",
    "visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n",
    "                            dataset_val.class_names, r['scores'], ax=get_ax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mAP:  0.8777777830759683\n"
     ]
    }
   ],
   "source": [
    "# Compute VOC-Style mAP @ IoU=0.5\n",
    "# Running on 10 images. Increase for better accuracy.\n",
    "image_ids = np.random.choice(dataset_val.image_ids, 10)\n",
    "APs = []\n",
    "for image_id in image_ids:\n",
    "    # Load image and ground truth data\n",
    "    image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "        modellib.load_image_gt(dataset_val, inference_config,\n",
    "                               image_id, use_mini_mask=False)\n",
    "    molded_images = np.expand_dims(modellib.mold_image(image, inference_config), 0)\n",
    "    # Run object detection\n",
    "    results = model.detect([image], verbose=0)\n",
    "    r = results[0]\n",
    "    # Compute AP\n",
    "    AP, precisions, recalls, overlaps =\\\n",
    "        utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
    "                         r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'])\n",
    "    APs.append(AP)\n",
    "    \n",
    "print(\"mAP: \", np.mean(APs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
